{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Default Prediction - Model Training Pipeline\n",
    "\n",
    "**Objective**: Train XGBoost model to predict credit default\n",
    "\n",
    "**Pipeline Configuration**:\n",
    "- Data Split: 70% train / 15% validation / 15% test + 1 month OOT\n",
    "- Class Imbalance: Class weights\n",
    "- Model: XGBoost with RandomizedSearchCV\n",
    "- Features: All features, then feature importance analysis\n",
    "\n",
    "**Date**: 2025-10-26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Spark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    f1_score, precision_score, recall_score, accuracy_score\n",
    ")\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Model persistence\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"model_training\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"Spark Session initialized\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Feature Store and Label Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature store (Aug 2023 onwards to avoid schema mismatch)\n",
    "feature_path = '/app/datamart/gold/feature_store/'\n",
    "\n",
    "print(\"Loading feature store (Aug 2023 onwards)...\")\n",
    "months_to_load = [\n",
    "    'gold_feature_store_2023_08_01.parquet',\n",
    "    'gold_feature_store_2023_09_01.parquet', \n",
    "    'gold_feature_store_2023_10_01.parquet',\n",
    "    'gold_feature_store_2023_11_01.parquet',\n",
    "    'gold_feature_store_2023_12_01.parquet',\n",
    "    'gold_feature_store_2024_01_01.parquet',\n",
    "    'gold_feature_store_2024_02_01.parquet',\n",
    "    'gold_feature_store_2024_03_01.parquet',\n",
    "    'gold_feature_store_2024_04_01.parquet',\n",
    "    'gold_feature_store_2024_05_01.parquet',\n",
    "    'gold_feature_store_2024_06_01.parquet',\n",
    "    'gold_feature_store_2024_07_01.parquet',\n",
    "    'gold_feature_store_2024_08_01.parquet',\n",
    "    'gold_feature_store_2024_09_01.parquet',\n",
    "    'gold_feature_store_2024_10_01.parquet',\n",
    "    'gold_feature_store_2024_11_01.parquet',\n",
    "    'gold_feature_store_2024_12_01.parquet',\n",
    "    'gold_feature_store_2025_01_01.parquet'\n",
    "]\n",
    "\n",
    "# Load and union all dataframes\n",
    "df_list = []\n",
    "for month_file in months_to_load:\n",
    "    file_path = f'{feature_path}{month_file}'\n",
    "    df_month = spark.read.parquet(file_path)\n",
    "    df_list.append(df_month)\n",
    "\n",
    "df_features_spark = df_list[0]\n",
    "for df_month in df_list[1:]:\n",
    "    df_features_spark = df_features_spark.union(df_month)\n",
    "\n",
    "print(f\"Feature store loaded: {df_features_spark.count():,} rows\")\n",
    "print(f\"Columns: {len(df_features_spark.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load label store\n",
    "label_path = '/app/datamart/gold/label_store/'\n",
    "\n",
    "print(\"Loading label store...\")\n",
    "df_labels_spark = spark.read.parquet(f'{label_path}*/*.parquet')\n",
    "\n",
    "print(f\"Label store loaded: {df_labels_spark.count():,} rows\")\n",
    "print(f\"\\nLabel store schema:\")\n",
    "df_labels_spark.printSchema()\n",
    "\n",
    "# Check label distribution\n",
    "print(\"\\nLabel distribution:\")\n",
    "df_labels_spark.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merge Feature Store with Label Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge feature store with label store\n",
    "# STRATEGY (following CreditKarma reference):\n",
    "# 1. Labels are filtered by date range (customers with loans at MOB=6)\n",
    "# 2. Features are matched to those customers (on Customer_ID only, ignoring snapshot_date)\n",
    "# 3. Each customer has ONE feature snapshot (from whenever it was captured)\n",
    "# 4. Each customer may have MULTIPLE labels (multiple loans at different MOB=6 dates)\n",
    "\n",
    "print(\"Merging feature store with label store...\")\n",
    "print(f\"Features before merge: {df_features_spark.count():,} rows\")\n",
    "print(f\"Labels before merge: {df_labels_spark.count():,} rows\")\n",
    "\n",
    "# Convert to Pandas for easier manipulation (following CreditKarma approach)\n",
    "df_features = df_features_spark.toPandas()\n",
    "df_labels = df_labels_spark.toPandas()\n",
    "\n",
    "print(f\"\\nFeature store: {df_features['Customer_ID'].nunique():,} unique customers\")\n",
    "print(f\"Label store: {df_labels['Customer_ID'].nunique():,} unique customers\")\n",
    "\n",
    "# Merge on Customer_ID only (NOT on snapshot_date)\n",
    "# This matches each customer's single feature snapshot with all their loan labels\n",
    "df = df_labels.merge(\n",
    "    df_features,\n",
    "    on='Customer_ID',\n",
    "    how='inner',\n",
    "    suffixes=('_label', '_feature')\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"MERGE RESULTS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Merged records: {len(df):,}\")\n",
    "print(f\"Unique customers: {df['Customer_ID'].nunique():,}\")\n",
    "\n",
    "if len(df) == 0:\n",
    "    print(\"\\n❌ ERROR: Merge produced 0 rows!\")\n",
    "    print(\"No common Customer_IDs between features and labels\")\n",
    "    raise ValueError(\"Merge failed - check data alignment\")\n",
    "\n",
    "# Rename snapshot_date columns for clarity\n",
    "df = df.rename(columns={\n",
    "    'snapshot_date_label': 'label_date',  # When loan reached MOB=6\n",
    "    'snapshot_date_feature': 'feature_date'  # When features were captured\n",
    "})\n",
    "\n",
    "# Use label_date as the primary snapshot_date for temporal splitting\n",
    "df['snapshot_date'] = pd.to_datetime(df['label_date'])\n",
    "\n",
    "print(f\"\\nDate range (label dates): {df['snapshot_date'].min()} to {df['snapshot_date'].max()}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nClass imbalance ratio:\")\n",
    "print(df['label'].value_counts(normalize=True))\n",
    "\n",
    "print(f\"\\n✅ Merge successful! Using CreditKarma strategy: merge on Customer_ID only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Temporal Data Split (70/15/15 + 1 month OOT)\n",
    "\n",
    "**Strategy**: Time-based split (NOT random) because this is temporal data\n",
    "- Train: Oldest 70%\n",
    "- Validation: Next 15%\n",
    "- Test: Next 15%\n",
    "- OOT (Out of Time): Most recent 1 month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by snapshot_date\n",
    "df = df.sort_values('snapshot_date').reset_index(drop=True)\n",
    "\n",
    "# Get unique sorted dates\n",
    "unique_dates = sorted(df['snapshot_date'].unique())\n",
    "print(f\"Total unique snapshot dates: {len(unique_dates)}\")\n",
    "print(f\"Date range: {unique_dates[0]} to {unique_dates[-1]}\")\n",
    "\n",
    "# Reserve last 1 month for OOT\n",
    "oot_date = unique_dates[-1]\n",
    "dates_for_split = unique_dates[:-1]  # Exclude last month\n",
    "\n",
    "print(f\"\\nOOT month: {oot_date}\")\n",
    "print(f\"Dates for train/val/test split: {len(dates_for_split)} months\")\n",
    "\n",
    "# Calculate split indices for remaining dates\n",
    "n_dates = len(dates_for_split)\n",
    "train_end_idx = int(n_dates * 0.70)\n",
    "val_end_idx = int(n_dates * 0.85)\n",
    "\n",
    "# Get split dates\n",
    "train_dates = dates_for_split[:train_end_idx]\n",
    "val_dates = dates_for_split[train_end_idx:val_end_idx]\n",
    "test_dates = dates_for_split[val_end_idx:]\n",
    "\n",
    "print(f\"\\nTrain dates: {len(train_dates)} months ({train_dates[0]} to {train_dates[-1]})\")\n",
    "print(f\"Validation dates: {len(val_dates)} months ({val_dates[0]} to {val_dates[-1]})\")\n",
    "print(f\"Test dates: {len(test_dates)} months ({test_dates[0]} to {test_dates[-1]})\")\n",
    "print(f\"OOT date: 1 month ({oot_date})\")\n",
    "\n",
    "# Create splits\n",
    "train_df = df[df['snapshot_date'].isin(train_dates)].copy()\n",
    "val_df = df[df['snapshot_date'].isin(val_dates)].copy()\n",
    "test_df = df[df['snapshot_date'].isin(test_dates)].copy()\n",
    "oot_df = df[df['snapshot_date'] == oot_date].copy()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SPLIT SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Train: {len(train_df):,} rows ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Validation: {len(val_df):,} rows ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test: {len(test_df):,} rows ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"OOT: {len(oot_df):,} rows ({len(oot_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Total: {len(train_df)+len(val_df)+len(test_df)+len(oot_df):,} rows\")\n",
    "\n",
    "# Check label distribution in each split\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"LABEL DISTRIBUTION BY SPLIT\")\n",
    "print(f\"{'='*80}\")\n",
    "for name, split_df in [('Train', train_df), ('Validation', val_df), ('Test', test_df), ('OOT', oot_df)]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(split_df['label'].value_counts())\n",
    "    print(f\"Default rate: {split_df['label'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature columns (exclude ID, date, and label)\n",
    "exclude_cols = ['Customer_ID', 'snapshot_date', 'label', 'last_activity_date']\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "print(f\"\\nFeature columns:\")\n",
    "print(feature_cols)\n",
    "\n",
    "# Separate numeric and categorical features\n",
    "numeric_features = train_df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = train_df[feature_cols].select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumeric features: {len(numeric_features)}\")\n",
    "print(numeric_features)\n",
    "print(f\"\\nCategorical features: {len(categorical_features)}\")\n",
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in training set:\")\n",
    "missing = train_df[feature_cols].isnull().sum()\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "if len(missing) > 0:\n",
    "    print(missing)\n",
    "else:\n",
    "    print(\"No missing values!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_data(train, val, test, oot, numeric_features, categorical_features):\n",
    "    \"\"\"\n",
    "    Preprocess data:\n",
    "    1. Handle missing values\n",
    "    2. Encode categorical features\n",
    "    3. Scale numeric features\n",
    "    \n",
    "    Returns: X_train, X_val, X_test, X_oot, y_train, y_val, y_test, y_oot, preprocessors\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make copies\n",
    "    train_proc = train.copy()\n",
    "    val_proc = val.copy()\n",
    "    test_proc = test.copy()\n",
    "    oot_proc = oot.copy()\n",
    "    \n",
    "    # 1. Handle missing values in numeric features (fill with median from training)\n",
    "    numeric_medians = {}\n",
    "    for col in numeric_features:\n",
    "        median_val = train_proc[col].median()\n",
    "        numeric_medians[col] = median_val\n",
    "        \n",
    "        train_proc[col].fillna(median_val, inplace=True)\n",
    "        val_proc[col].fillna(median_val, inplace=True)\n",
    "        test_proc[col].fillna(median_val, inplace=True)\n",
    "        oot_proc[col].fillna(median_val, inplace=True)\n",
    "    \n",
    "    # 2. Encode categorical features (Label Encoding)\n",
    "    label_encoders = {}\n",
    "    for col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        \n",
    "        # Fill missing categorical with 'Unknown'\n",
    "        train_proc[col].fillna('Unknown', inplace=True)\n",
    "        val_proc[col].fillna('Unknown', inplace=True)\n",
    "        test_proc[col].fillna('Unknown', inplace=True)\n",
    "        oot_proc[col].fillna('Unknown', inplace=True)\n",
    "        \n",
    "        # Fit on training data\n",
    "        le.fit(train_proc[col])\n",
    "        label_encoders[col] = le\n",
    "        \n",
    "        # Transform all splits\n",
    "        train_proc[col] = le.transform(train_proc[col])\n",
    "        \n",
    "        # Handle unseen categories in val/test/oot\n",
    "        for df_proc in [val_proc, test_proc, oot_proc]:\n",
    "            df_proc[col] = df_proc[col].apply(\n",
    "                lambda x: x if x in le.classes_ else 'Unknown'\n",
    "            )\n",
    "            df_proc[col] = le.transform(df_proc[col])\n",
    "    \n",
    "    # 3. Scale numeric features (StandardScaler)\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit on training data\n",
    "    scaler.fit(train_proc[numeric_features])\n",
    "    \n",
    "    # Transform all splits\n",
    "    train_proc[numeric_features] = scaler.transform(train_proc[numeric_features])\n",
    "    val_proc[numeric_features] = scaler.transform(val_proc[numeric_features])\n",
    "    test_proc[numeric_features] = scaler.transform(test_proc[numeric_features])\n",
    "    oot_proc[numeric_features] = scaler.transform(oot_proc[numeric_features])\n",
    "    \n",
    "    # Create X and y\n",
    "    all_features = numeric_features + categorical_features\n",
    "    \n",
    "    X_train = train_proc[all_features]\n",
    "    X_val = val_proc[all_features]\n",
    "    X_test = test_proc[all_features]\n",
    "    X_oot = oot_proc[all_features]\n",
    "    \n",
    "    y_train = train_proc['label']\n",
    "    y_val = val_proc['label']\n",
    "    y_test = test_proc['label']\n",
    "    y_oot = oot_proc['label']\n",
    "    \n",
    "    # Store preprocessors\n",
    "    preprocessors = {\n",
    "        'numeric_medians': numeric_medians,\n",
    "        'label_encoders': label_encoders,\n",
    "        'scaler': scaler,\n",
    "        'feature_names': all_features,\n",
    "        'numeric_features': numeric_features,\n",
    "        'categorical_features': categorical_features\n",
    "    }\n",
    "    \n",
    "    return X_train, X_val, X_test, X_oot, y_train, y_val, y_test, y_oot, preprocessors\n",
    "\n",
    "print(\"Preprocessing data...\")\n",
    "X_train, X_val, X_test, X_oot, y_train, y_val, y_test, y_oot, preprocessors = preprocess_data(\n",
    "    train_df, val_df, test_df, oot_df, numeric_features, categorical_features\n",
    ")\n",
    "\n",
    "print(f\"\\nPreprocessing complete!\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"X_oot shape: {X_oot.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights to handle imbalance\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "# Create dictionary for XGBoost\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights_array)}\n",
    "\n",
    "print(\"Class weights (to handle imbalance):\")\n",
    "print(class_weights_dict)\n",
    "\n",
    "# Calculate scale_pos_weight for XGBoost (alternative to class weights)\n",
    "n_neg = (y_train == 0).sum()\n",
    "n_pos = (y_train == 1).sum()\n",
    "scale_pos_weight = n_neg / n_pos\n",
    "\n",
    "print(f\"\\nScale pos weight: {scale_pos_weight:.2f}\")\n",
    "print(f\"  (Ratio of negative to positive samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Baseline XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline XGBoost with default parameters + class weights\n",
    "print(\"Training baseline XGBoost model...\")\n",
    "\n",
    "baseline_model = XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,  # Handle class imbalance\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    early_stopping_rounds=10,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "# Train with early stopping on validation set\n",
    "baseline_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Baseline model trained!\")\n",
    "print(f\"Best iteration: {baseline_model.best_iteration}\")\n",
    "print(f\"Best score: {baseline_model.best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline model\n",
    "def evaluate_model(model, X, y, dataset_name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Evaluate model and print comprehensive metrics\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{dataset_name.upper()} METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Classification metrics\n",
    "    print(f\"\\nAccuracy: {accuracy_score(y, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y, y_pred):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y, y_pred):.4f}\")\n",
    "    print(f\"F1-Score: {f1_score(y, y_pred):.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y, y_pred_proba):.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y, y_pred))\n",
    "    \n",
    "    return y_pred, y_pred_proba\n",
    "\n",
    "# Evaluate on all datasets\n",
    "y_train_pred, y_train_proba = evaluate_model(baseline_model, X_train, y_train, \"Train\")\n",
    "y_val_pred, y_val_proba = evaluate_model(baseline_model, X_val, y_val, \"Validation\")\n",
    "y_test_pred, y_test_proba = evaluate_model(baseline_model, X_test, y_test, \"Test\")\n",
    "y_oot_pred, y_oot_proba = evaluate_model(baseline_model, X_oot, y_oot, \"OOT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Tuning with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter search space\n",
    "param_distributions = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [3, 5, 7, 9, 11],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'min_child_weight': [1, 3, 5, 7],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'reg_alpha': [0, 0.01, 0.1, 1],\n",
    "    'reg_lambda': [1, 1.5, 2, 3]\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter search space:\")\n",
    "for param, values in param_distributions.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "print(f\"\\nTotal possible combinations: {np.prod([len(v) for v in param_distributions.values()]):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV\n",
    "print(\"Starting RandomizedSearchCV...\")\n",
    "print(\"This may take several minutes...\\n\")\n",
    "\n",
    "# Create XGBoost model WITHOUT early_stopping_rounds for CV\n",
    "# (Early stopping requires eval_set which isn't available during CV)\n",
    "xgb_model = XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    "    # NOTE: No early_stopping_rounds here - will add after CV\n",
    ")\n",
    "\n",
    "# Use StratifiedKFold for cross-validation (maintains class distribution)\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,  # Number of random combinations to try\n",
    "    cv=cv,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nBest ROC-AUC (CV): {random_search.best_score_:.4f}\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Retrain best model with early stopping on validation set\n",
    "print(\"Retraining best model with early stopping on validation set...\")\n",
    "\n",
    "best_model_retrained = XGBClassifier(\n",
    "    **random_search.best_params_,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "best_model_retrained.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"Best model retrained!\")\n",
    "print(f\"Best iteration: {best_model_retrained.best_iteration}\")\n",
    "print(f\"Best score: {best_model_retrained.best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate Final Model on All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned model on all datasets\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "y_train_pred_final, y_train_proba_final = evaluate_model(best_model_retrained, X_train, y_train, \"Train\")\n",
    "y_val_pred_final, y_val_proba_final = evaluate_model(best_model_retrained, X_val, y_val, \"Validation\")\n",
    "y_test_pred_final, y_test_proba_final = evaluate_model(best_model_retrained, X_test, y_test, \"Test\")\n",
    "y_oot_pred_final, y_oot_proba_final = evaluate_model(best_model_retrained, X_oot, y_oot, \"OOT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs tuned model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE vs TUNED MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for dataset_name, X, y in [\n",
    "    ('Train', X_train, y_train),\n",
    "    ('Validation', X_val, y_val),\n",
    "    ('Test', X_test, y_test),\n",
    "    ('OOT', X_oot, y_oot)\n",
    "]:\n",
    "    # Baseline\n",
    "    y_pred_base = baseline_model.predict(X)\n",
    "    y_proba_base = baseline_model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # Tuned\n",
    "    y_pred_tuned = best_model_retrained.predict(X)\n",
    "    y_proba_tuned = best_model_retrained.predict_proba(X)[:, 1]\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'Dataset': dataset_name,\n",
    "        'Baseline_AUC': roc_auc_score(y, y_proba_base),\n",
    "        'Tuned_AUC': roc_auc_score(y, y_proba_tuned),\n",
    "        'Baseline_F1': f1_score(y, y_pred_base),\n",
    "        'Tuned_F1': f1_score(y, y_pred_tuned),\n",
    "        'Baseline_Precision': precision_score(y, y_pred_base),\n",
    "        'Tuned_Precision': precision_score(y, y_pred_tuned),\n",
    "        'Baseline_Recall': recall_score(y, y_pred_base),\n",
    "        'Tuned_Recall': recall_score(y, y_pred_tuned)\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Calculate improvements\n",
    "print(f\"\\nAverage AUC improvement: {(comparison_df['Tuned_AUC'] - comparison_df['Baseline_AUC']).mean():.4f}\")\n",
    "print(f\"Average F1 improvement: {(comparison_df['Tuned_F1'] - comparison_df['Baseline_F1']).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from best model\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': preprocessors['feature_names'],\n",
    "    'importance': best_model_retrained.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE (Top 20)\")\n",
    "print(\"=\"*80)\n",
    "print(feature_importance.head(20).to_string(index=False))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_20 = feature_importance.head(20)\n",
    "plt.barh(range(len(top_20)), top_20['importance'])\n",
    "plt.yticks(range(len(top_20)), top_20['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 20 Feature Importance (XGBoost)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cumulative importance\n",
    "feature_importance['cumulative_importance'] = feature_importance['importance'].cumsum() / feature_importance['importance'].sum()\n",
    "\n",
    "# How many features to reach 80% importance?\n",
    "n_features_80 = (feature_importance['cumulative_importance'] <= 0.80).sum()\n",
    "print(f\"\\nNumber of features to reach 80% cumulative importance: {n_features_80}/{len(feature_importance)}\")\n",
    "\n",
    "# Show those features\n",
    "print(f\"\\nTop {n_features_80} features (80% importance):\")\n",
    "print(feature_importance.head(n_features_80)['feature'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ROC Curve and Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all datasets\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "datasets = [\n",
    "    ('Train', y_train, y_train_proba_final),\n",
    "    ('Validation', y_val, y_val_proba_final),\n",
    "    ('Test', y_test, y_test_proba_final),\n",
    "    ('OOT', y_oot, y_oot_proba_final)\n",
    "]\n",
    "\n",
    "for idx, (name, y_true, y_proba) in enumerate(datasets):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    \n",
    "    axes[idx].plot(fpr, tpr, label=f'ROC (AUC = {auc:.4f})', linewidth=2)\n",
    "    axes[idx].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    axes[idx].set_xlabel('False Positive Rate')\n",
    "    axes[idx].set_ylabel('True Positive Rate')\n",
    "    axes[idx].set_title(f'ROC Curve - {name}')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curves for all datasets\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, y_true, y_proba) in enumerate(datasets):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_proba)\n",
    "    \n",
    "    axes[idx].plot(recall, precision, linewidth=2)\n",
    "    axes[idx].set_xlabel('Recall')\n",
    "    axes[idx].set_ylabel('Precision')\n",
    "    axes[idx].set_title(f'Precision-Recall Curve - {name}')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add baseline (random classifier)\n",
    "    baseline = y_true.sum() / len(y_true)\n",
    "    axes[idx].axhline(y=baseline, color='k', linestyle='--', label=f'Baseline ({baseline:.3f})')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Model and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "import os\n",
    "os.makedirs('/app/models', exist_ok=True)\n",
    "\n",
    "# Save final model\n",
    "model_path = '/app/models/xgboost_credit_default_model.pkl'\n",
    "joblib.dump(best_model_retrained, model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save preprocessors\n",
    "preprocessor_path = '/app/models/preprocessors.pkl'\n",
    "joblib.dump(preprocessors, preprocessor_path)\n",
    "print(f\"Preprocessors saved to: {preprocessor_path}\")\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance_path = '/app/models/feature_importance.csv'\n",
    "feature_importance.to_csv(feature_importance_path, index=False)\n",
    "print(f\"Feature importance saved to: {feature_importance_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_type': 'XGBoost',\n",
    "    'training_date': str(datetime.now()),\n",
    "    'train_size': len(X_train),\n",
    "    'val_size': len(X_val),\n",
    "    'test_size': len(X_test),\n",
    "    'oot_size': len(X_oot),\n",
    "    'n_features': len(preprocessors['feature_names']),\n",
    "    'best_params': random_search.best_params_,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'train_auc': roc_auc_score(y_train, y_train_proba_final),\n",
    "    'val_auc': roc_auc_score(y_val, y_val_proba_final),\n",
    "    'test_auc': roc_auc_score(y_test, y_test_proba_final),\n",
    "    'oot_auc': roc_auc_score(y_oot, y_oot_proba_final),\n",
    "    'train_f1': f1_score(y_train, y_train_pred_final),\n",
    "    'val_f1': f1_score(y_val, y_val_pred_final),\n",
    "    'test_f1': f1_score(y_test, y_test_pred_final),\n",
    "    'oot_f1': f1_score(y_oot, y_oot_pred_final)\n",
    "}\n",
    "\n",
    "metadata_path = '/app/models/model_metadata.pkl'\n",
    "joblib.dump(metadata, metadata_path)\n",
    "print(f\"Model metadata saved to: {metadata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFinal Model Performance:\")\n",
    "print(f\"  Test AUC: {metadata['test_auc']:.4f}\")\n",
    "print(f\"  Test F1: {metadata['test_f1']:.4f}\")\n",
    "print(f\"  OOT AUC: {metadata['oot_auc']:.4f}\")\n",
    "print(f\"  OOT F1: {metadata['oot_f1']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
