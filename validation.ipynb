{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline Validation\n",
    "\n",
    "This notebook verifies that all fixes have been successfully applied after the Airflow DAG rerun.\n",
    "\n",
    "**Fixes to Verify:**\n",
    "1. Num_Credit_Inquiries: 100% NULL → ~5% NULL\n",
    "2. Age: string dtype → integer dtype\n",
    "3. Outlier bounds: 7 new columns cleaned\n",
    "4. Clickstream: Conditional fill (NULL for Jan 2023, 0 for non-clickers)\n",
    "\n",
    "**Date**: 2025-10-26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/26 09:01:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session initialized\n",
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"validation\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"Spark Session initialized\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Gold Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading months with clickstream data)...\n",
      "================================================================================\n",
      "GOLD FEATURE STORE - LOADED (Aug 2023 onwards)\n",
      "================================================================================\n",
      "Total rows: 12,500\n",
      "Total columns: 30\n",
      "Date range: 2023-01-01 to 2025-01-01\n"
     ]
    }
   ],
   "source": [
    "# Load gold feature store - Load only recent months to avoid schema mismatch\n",
    "# NOTE: Old parquet files have INT/FLOAT schema, new ones have LONG/DOUBLE\n",
    "# After fixing gold_layer.py schema, we'll need to rerun the DAG\n",
    "# For now, let's just load files from later months that have actual clickstream data\n",
    "\n",
    "feature_path = '/app/datamart/gold/feature_store/'\n",
    "\n",
    "# Option 1: Load just one month to verify schema\n",
    "print(\"Loading months with clickstream data)...\")\n",
    "months_to_load = [\n",
    "    'gold_feature_store_2023_01_01.parquet',\n",
    "    'gold_feature_store_2023_02_01.parquet',\n",
    "    'gold_feature_store_2023_03_01.parquet',\n",
    "    'gold_feature_store_2023_04_01.parquet',\n",
    "    'gold_feature_store_2023_05_01.parquet',\n",
    "    'gold_feature_store_2023_06_01.parquet',\n",
    "    'gold_feature_store_2023_07_01.parquet',\n",
    "    'gold_feature_store_2023_08_01.parquet',\n",
    "    'gold_feature_store_2023_09_01.parquet', \n",
    "    'gold_feature_store_2023_10_01.parquet',\n",
    "    'gold_feature_store_2023_11_01.parquet',\n",
    "    'gold_feature_store_2023_12_01.parquet',\n",
    "    'gold_feature_store_2024_01_01.parquet',\n",
    "    'gold_feature_store_2024_02_01.parquet',\n",
    "    'gold_feature_store_2024_03_01.parquet',\n",
    "    'gold_feature_store_2024_04_01.parquet',\n",
    "    'gold_feature_store_2024_05_01.parquet',\n",
    "    'gold_feature_store_2024_06_01.parquet',\n",
    "    'gold_feature_store_2024_07_01.parquet',\n",
    "    'gold_feature_store_2024_08_01.parquet',\n",
    "    'gold_feature_store_2024_09_01.parquet',\n",
    "    'gold_feature_store_2024_10_01.parquet',\n",
    "    'gold_feature_store_2024_11_01.parquet',\n",
    "    'gold_feature_store_2024_12_01.parquet',\n",
    "    'gold_feature_store_2025_01_01.parquet'\n",
    "]\n",
    "\n",
    "# Load only months with consistent schema\n",
    "df_list = []\n",
    "for month_file in months_to_load:\n",
    "    file_path = f'{feature_path}{month_file}'\n",
    "    df_month = spark.read.parquet(file_path)\n",
    "    df_list.append(df_month)\n",
    "\n",
    "# Union all dataframes\n",
    "from pyspark.sql import DataFrame\n",
    "df_spark = df_list[0]\n",
    "for df_month in df_list[1:]:\n",
    "    df_spark = df_spark.union(df_month)\n",
    "\n",
    "df = df_spark.toPandas()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GOLD FEATURE STORE - LOADED (Aug 2023 onwards)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(f\"Date range: {df['snapshot_date'].min()} to {df['snapshot_date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify Fix #1: Num_Credit_Inquiries\n",
    "\n",
    "**Expected**: Should go from 100% NULL to ~5% NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FIX #1: Num_Credit_Inquiries\n",
      "================================================================================\n",
      "\n",
      "Null Count: 192 / 12,500 (1.5%)\n",
      "✅ PASS: Num_Credit_Inquiries is now populated!\n",
      "\n",
      "Sample values:\n",
      "[3.0, 5.0, 0.0, 4.0, 11.0, 3.0, 8.0, 2.0, 9.0, 5.0, 2.0, 6.0, 2.0, 4.0, 6.0, 9.0, 10.0, 2.0, 5.0, 6.0]\n",
      "\n",
      "Stats:\n",
      "  Min: 0.0\n",
      "  Max: 46.0\n",
      "  Mean: 6.64\n",
      "  Median: 6.00\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FIX #1: Num_Credit_Inquiries\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'Num_Credit_Inquiries' in df.columns:\n",
    "    nci_null = df['Num_Credit_Inquiries'].isnull().sum()\n",
    "    nci_total = len(df)\n",
    "    nci_null_pct = (nci_null / nci_total * 100)\n",
    "    \n",
    "    print(f\"\\nNull Count: {nci_null:,} / {nci_total:,} ({nci_null_pct:.1f}%)\")\n",
    "    \n",
    "    if nci_null_pct < 10:\n",
    "        print(\"✅ PASS: Num_Credit_Inquiries is now populated!\")\n",
    "        print(f\"\\nSample values:\")\n",
    "        print(df['Num_Credit_Inquiries'].dropna().head(20).tolist())\n",
    "        print(f\"\\nStats:\")\n",
    "        print(f\"  Min: {df['Num_Credit_Inquiries'].min()}\")\n",
    "        print(f\"  Max: {df['Num_Credit_Inquiries'].max()}\")\n",
    "        print(f\"  Mean: {df['Num_Credit_Inquiries'].mean():.2f}\")\n",
    "        print(f\"  Median: {df['Num_Credit_Inquiries'].median():.2f}\")\n",
    "    elif nci_null_pct == 100:\n",
    "        print(\"❌ FAIL: Still 100% NULL - Fix not applied!\")\n",
    "    else:\n",
    "        print(f\"⚠️  PARTIAL: {nci_null_pct:.1f}% NULL (expected < 10%)\")\n",
    "else:\n",
    "    print(\"❌ FAIL: Num_Credit_Inquiries column not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Fix #2: Age Dtype\n",
    "\n",
    "**Expected**: Should be integer dtype (not string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FIX #2: Age Dtype (Verified on Spark DataFrame)\n",
      "================================================================================\n",
      "\n",
      "Age dtype: IntegerType()\n",
      "✅ PASS: Age is now integer/long dtype!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null Count: 601 / 12,500 (4.8%)\n",
      "\n",
      "Stats:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:===================================>                    (16 + 9) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Min: 16\n",
      "  Max: 56\n",
      "  Mean: 34.1\n",
      "  Median: 34.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, LongType, StringType, DoubleType, FloatType\n",
    "from pyspark.sql.functions import col, isnan, when, count, min, max, avg, percentile_approx\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FIX #2: Age Dtype (Verified on Spark DataFrame)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'Age' in df_spark.columns:\n",
    "    # --- 1. Check Dtype ---\n",
    "    # Get the data type from the Spark schema\n",
    "    age_dtype = df_spark.schema['Age'].dataType\n",
    "    print(f\"\\nAge dtype: {age_dtype}\")\n",
    "    \n",
    "    # Check if the type is one of Spark's integer types\n",
    "    if isinstance(age_dtype, (IntegerType, LongType)):\n",
    "        print(\"✅ PASS: Age is now integer/long dtype!\")\n",
    "        is_numeric = True\n",
    "    elif isinstance(age_dtype, StringType):\n",
    "        print(\"❌ FAIL: Age is still string dtype - Fix not applied!\")\n",
    "        is_numeric = False\n",
    "    else:\n",
    "        print(f\"⚠️  UNEXPECTED: Age dtype is {age_dtype}\")\n",
    "        is_numeric = isinstance(age_dtype, (DoubleType, FloatType)) # Still numeric, but not integer\n",
    "\n",
    "    # --- 2. Calculate Nulls (Requires Spark Actions) ---\n",
    "    total_count = df_spark.count()\n",
    "    age_null = df_spark.filter(col('Age').isNull()).count()\n",
    "    age_null_pct = (age_null / total_count * 100) if total_count > 0 else 0\n",
    "    print(f\"\\nNull Count: {age_null:,} / {total_count:,} ({age_null_pct:.1f}%)\")\n",
    "    \n",
    "    # --- 3. Calculate Stats (Requires Spark Actions) ---\n",
    "    if age_null < total_count and is_numeric:\n",
    "        print(f\"\\nStats:\")\n",
    "        try:\n",
    "            # Calculate stats in a single pass for efficiency\n",
    "            stats = df_spark.select(\n",
    "                min('Age').alias('Min'),\n",
    "                max('Age').alias('Max'),\n",
    "                avg('Age').alias('Mean'),\n",
    "                percentile_approx('Age', 0.5).alias('Median') # Median is an approximation in Spark\n",
    "            ).first()\n",
    "            \n",
    "            print(f\"  Min: {stats['Min']}\")\n",
    "            print(f\"  Max: {stats['Max']}\")\n",
    "            print(f\"  Mean: {stats['Mean']:.1f}\")\n",
    "            print(f\"  Median: {stats['Median']:.1f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Unable to calculate stats (column may be non-numeric): {e}\")\n",
    "    elif not is_numeric:\n",
    "         print(\"\\nStats: Skipped (column is not numeric)\")\n",
    "else:\n",
    "    print(\"❌ FAIL: Age column not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Fix #3: Outlier Bounds\n",
    "\n",
    "**Expected**: 7 new columns should have outliers removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FIX #3: Outlier Bounds\n",
      "================================================================================\n",
      "\n",
      "Outlier Bounds Validation:\n",
      "                 Column   Expected_Range Actual_Min Actual_Max  Outside_Bounds Status\n",
      "            Num_of_Loan          (0, 20)       0.00      18.00               0 ✅ PASS\n",
      " Num_of_Delayed_Payment         (0, 100)       0.00      88.00               0 ✅ PASS\n",
      "          Annual_Income     (0, 1000000)    7005.93  580744.00               0 ✅ PASS\n",
      "Amount_invested_monthly       (0, 10000)       0.00   10000.00               0 ✅ PASS\n",
      "        Monthly_Balance (-10000, 100000)       0.38    1463.79               0 ✅ PASS\n",
      "       Outstanding_Debt       (0, 50000)       0.23    4998.07               0 ✅ PASS\n",
      "    Delay_from_due_date         (0, 365)       0.00      67.00               0 ✅ PASS\n",
      "\n",
      "================================================================================\n",
      "Summary: 7/7 columns passed outlier bounds check\n",
      "✅ ALL OUTLIER BOUNDS APPLIED SUCCESSFULLY!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FIX #3: Outlier Bounds\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define expected bounds\n",
    "outlier_bounds = {\n",
    "    \"Num_of_Loan\": (0, 20),\n",
    "    \"Num_of_Delayed_Payment\": (0, 100),\n",
    "    \"Annual_Income\": (0, 1000000),\n",
    "    \"Amount_invested_monthly\": (0, 10000),\n",
    "    \"Monthly_Balance\": (-10000, 100000),\n",
    "    \"Outstanding_Debt\": (0, 50000),\n",
    "    \"Delay_from_due_date\": (0, 365)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for col_name, (min_val, max_val) in outlier_bounds.items():\n",
    "    if col_name in df.columns:\n",
    "        non_null = df[col_name].dropna()\n",
    "        if len(non_null) > 0:\n",
    "            outside_bounds = ((non_null < min_val) | (non_null > max_val)).sum()\n",
    "            actual_min = non_null.min()\n",
    "            actual_max = non_null.max()\n",
    "            \n",
    "            status = \"✅ PASS\" if outside_bounds == 0 else \"❌ FAIL\"\n",
    "            \n",
    "            results.append({\n",
    "                'Column': col_name,\n",
    "                'Expected_Range': f\"({min_val}, {max_val})\",\n",
    "                'Actual_Min': f\"{actual_min:.2f}\",\n",
    "                'Actual_Max': f\"{actual_max:.2f}\",\n",
    "                'Outside_Bounds': outside_bounds,\n",
    "                'Status': status\n",
    "            })\n",
    "        else:\n",
    "            results.append({\n",
    "                'Column': col_name,\n",
    "                'Expected_Range': f\"({min_val}, {max_val})\",\n",
    "                'Actual_Min': 'N/A',\n",
    "                'Actual_Max': 'N/A',\n",
    "                'Outside_Bounds': 0,\n",
    "                'Status': '⚠️  ALL NULL'\n",
    "            })\n",
    "    else:\n",
    "        results.append({\n",
    "            'Column': col_name,\n",
    "            'Expected_Range': f\"({min_val}, {max_val})\",\n",
    "            'Actual_Min': 'N/A',\n",
    "            'Actual_Max': 'N/A',\n",
    "            'Outside_Bounds': 0,\n",
    "            'Status': '❌ NOT FOUND'\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\nOutlier Bounds Validation:\")\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# Summary\n",
    "passed = len([r for r in results if r['Status'] == '✅ PASS'])\n",
    "total = len(results)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Summary: {passed}/{total} columns passed outlier bounds check\")\n",
    "if passed == total:\n",
    "    print(\"✅ ALL OUTLIER BOUNDS APPLIED SUCCESSFULLY!\")\n",
    "else:\n",
    "    print(\"⚠️  Some columns still have outliers or are missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Fix #4: Clickstream Conditional Fill\n",
    "\n",
    "**Expected**: \n",
    "- Jan 2023: Should have NULL values (no Dec 2022 data)\n",
    "- Other months: Should have 0 for non-clickers (not NULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FIX #4: Clickstream Conditional Fill\n",
      "================================================================================\n",
      "\n",
      "Clickstream NULL % by Month (visit_frequency):\n",
      "               visit_frequency\n",
      "snapshot_date                 \n",
      "2023-01                  100.0\n",
      "2023-02                    0.0\n",
      "2023-03                    0.0\n",
      "2023-04                    0.0\n",
      "2023-05                    0.0\n",
      "2023-06                    0.0\n",
      "2023-07                    0.0\n",
      "2023-08                    0.0\n",
      "2023-09                    0.0\n",
      "2023-10                    0.0\n",
      "2023-11                    0.0\n",
      "2023-12                    0.0\n",
      "\n",
      "Jan 2023:\n",
      "  NULL %: 100.0%\n",
      "  ✅ PASS: Jan 2023 has 100% NULL (expected - no Dec 2022 data)\n",
      "\n",
      "Feb 2023 onwards:\n",
      "  NULL %: 0.0%\n",
      "  ✅ PASS: Very few NULLs (filled with 0 for non-clickers)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FIX #4: Clickstream Conditional Fill\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "clickstream_cols = ['visit_frequency', 'recent_total_activity', 'avg_activity_intensity', 'total_active_features']\n",
    "\n",
    "# Check if columns exist\n",
    "existing_click_cols = [c for c in clickstream_cols if c in df.columns]\n",
    "\n",
    "if existing_click_cols:\n",
    "    # Convert snapshot_date to datetime\n",
    "    df['snapshot_date'] = pd.to_datetime(df['snapshot_date'])\n",
    "    \n",
    "    # Group by month and check NULL percentage\n",
    "    monthly_nulls = df.groupby(df['snapshot_date'].dt.to_period('M')).agg({\n",
    "        existing_click_cols[0]: lambda x: x.isnull().sum() / len(x) * 100\n",
    "    }).round(1)\n",
    "    \n",
    "    print(f\"\\nClickstream NULL % by Month ({existing_click_cols[0]}):\")\n",
    "    print(monthly_nulls.head(12))\n",
    "    \n",
    "    # Check Jan 2023\n",
    "    jan_2023 = df[df['snapshot_date'].dt.to_period('M') == '2023-01']\n",
    "    if len(jan_2023) > 0:\n",
    "        jan_null_pct = jan_2023[existing_click_cols[0]].isnull().sum() / len(jan_2023) * 100\n",
    "        print(f\"\\nJan 2023:\")\n",
    "        print(f\"  NULL %: {jan_null_pct:.1f}%\")\n",
    "        if jan_null_pct == 100:\n",
    "            print(\"  ✅ PASS: Jan 2023 has 100% NULL (expected - no Dec 2022 data)\")\n",
    "        else:\n",
    "            print(f\"  ⚠️  Expected 100% NULL, got {jan_null_pct:.1f}%\")\n",
    "    \n",
    "    # Check Feb 2023 onwards\n",
    "    feb_onwards = df[df['snapshot_date'] >= '2023-02-01']\n",
    "    if len(feb_onwards) > 0:\n",
    "        feb_null_pct = feb_onwards[existing_click_cols[0]].isnull().sum() / len(feb_onwards) * 100\n",
    "        print(f\"\\nFeb 2023 onwards:\")\n",
    "        print(f\"  NULL %: {feb_null_pct:.1f}%\")\n",
    "        if feb_null_pct < 5:\n",
    "            print(\"  ✅ PASS: Very few NULLs (filled with 0 for non-clickers)\")\n",
    "        elif feb_null_pct > 50:\n",
    "            print(\"  ❌ FAIL: Too many NULLs - conditional fill not applied!\")\n",
    "        else:\n",
    "            print(f\"  ⚠️  Some NULLs present ({feb_null_pct:.1f}%) - might be expected\")\n",
    "else:\n",
    "    print(\"❌ FAIL: No clickstream columns found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Overall Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "OVERALL DATA QUALITY SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Total rows: 12,500\n",
      "Total columns: 30\n",
      "Numeric columns: 22\n",
      "\n",
      "Top 10 Columns with Missing Values:\n",
      "                Column  Null_Count  Null_%\n",
      "                   Age         601   4.808\n",
      "           Num_of_Loan         566   4.528\n",
      " total_active_features         530   4.240\n",
      " recent_total_activity         530   4.240\n",
      "avg_activity_intensity         530   4.240\n",
      "       visit_frequency         530   4.240\n",
      "   Total_EMI_per_month         376   3.008\n",
      "       Num_Credit_Card         296   2.368\n",
      "         Interest_Rate         270   2.160\n",
      "  Changed_Credit_Limit         254   2.032\n",
      "\n",
      "Checking for Absurd Values:\n",
      "  ✅ No absurd values found!\n",
      "\n",
      "================================================================================\n",
      "VALIDATION COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"OVERALL DATA QUALITY SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get numeric columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "exclude_cols = ['snapshot_date', 'last_activity_date']\n",
    "numeric_cols = [c for c in numeric_cols if c not in exclude_cols]\n",
    "\n",
    "print(f\"\\nTotal rows: {len(df):,}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(f\"Numeric columns: {len(numeric_cols)}\")\n",
    "\n",
    "# Missing value summary\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': numeric_cols,\n",
    "    'Null_Count': [df[c].isnull().sum() for c in numeric_cols],\n",
    "    'Null_%': [(df[c].isnull().sum() / len(df) * 100) for c in numeric_cols]\n",
    "}).sort_values('Null_%', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Columns with Missing Values:\")\n",
    "print(missing_summary.head(10).to_string(index=False))\n",
    "\n",
    "# Check for extreme outliers still present\n",
    "print(\"\\nChecking for Absurd Values:\")\n",
    "absurd_found = False\n",
    "\n",
    "absurd_checks = {\n",
    "    'Monthly_Balance': (-1e10, 1e10),\n",
    "    'Annual_Income': (0, 10000000),\n",
    "    'Num_of_Loan': (0, 1000),\n",
    "    'Num_of_Delayed_Payment': (0, 1000)\n",
    "}\n",
    "\n",
    "for col_name, (min_sane, max_sane) in absurd_checks.items():\n",
    "    if col_name in df.columns:\n",
    "        absurd = df[(df[col_name] < min_sane) | (df[col_name] > max_sane)]\n",
    "        if len(absurd) > 0:\n",
    "            absurd_found = True\n",
    "            print(f\"  ⚠️  {col_name}: {len(absurd)} absurd values\")\n",
    "            print(f\"     Range: {df[col_name].min():.2f} to {df[col_name].max():.2f}\")\n",
    "\n",
    "if not absurd_found:\n",
    "    print(\"  ✅ No absurd values found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VALIDATION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Verdict\n",
    "\n",
    "Review the results above to confirm:\n",
    "- ✅ Fix #1: Num_Credit_Inquiries populated\n",
    "- ✅ Fix #2: Age is integer dtype\n",
    "- ✅ Fix #3: All 7 outlier bounds applied\n",
    "- ✅ Fix #4: Clickstream conditional fill working\n",
    "\n",
    "If all checks pass, your pipeline rerun was successful!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
